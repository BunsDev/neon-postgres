diff --git a/src/test/regress/expected/aggregates.out b/src/test/regress/expected/aggregates.out
index 68fd716d3e6..71ef6d60861 100644
--- a/src/test/regress/expected/aggregates.out
+++ b/src/test/regress/expected/aggregates.out
@@ -11,7 +11,8 @@ CREATE TABLE aggtest (
 	b			float4
 );
 \set filename :abs_srcdir '/data/agg.data'
-COPY aggtest FROM :'filename';
+\set command '\\copy aggtest FROM ' :'filename';
+:command
 ANALYZE aggtest;
 SELECT avg(four) AS avg_1 FROM onek;
        avg_1        
diff --git a/src/test/regress/expected/arrays.out b/src/test/regress/expected/arrays.out
index 957498432d6..1c8d93ae035 100644
--- a/src/test/regress/expected/arrays.out
+++ b/src/test/regress/expected/arrays.out
@@ -18,7 +18,8 @@ CREATE TABLE array_op_test (
 	t			text[]
 );
 \set filename :abs_srcdir '/data/array.data'
-COPY array_op_test FROM :'filename';
+\set command '\\copy array_op_test FROM ' :'filename';
+:command
 ANALYZE array_op_test;
 --
 -- only the 'e' array is 0-based, the others are 1-based.
diff --git a/src/test/regress/expected/btree_index.out b/src/test/regress/expected/btree_index.out
index 93ed5e8cc00..54bd7d535c4 100644
--- a/src/test/regress/expected/btree_index.out
+++ b/src/test/regress/expected/btree_index.out
@@ -20,13 +20,17 @@ CREATE TABLE bt_f8_heap (
 	random 		int4
 );
 \set filename :abs_srcdir '/data/desc.data'
-COPY bt_i4_heap FROM :'filename';
+\set command '\\copy bt_i4_heap FROM ' :'filename';
+:command
 \set filename :abs_srcdir '/data/hash.data'
-COPY bt_name_heap FROM :'filename';
+\set command '\\copy bt_name_heap FROM ' :'filename';
+:command
 \set filename :abs_srcdir '/data/desc.data'
-COPY bt_txt_heap FROM :'filename';
+\set command '\\copy bt_txt_heap FROM ' :'filename';
+:command
 \set filename :abs_srcdir '/data/hash.data'
-COPY bt_f8_heap FROM :'filename';
+\set command '\\copy bt_f8_heap FROM ' :'filename';
+:command
 ANALYZE bt_i4_heap;
 ANALYZE bt_name_heap;
 ANALYZE bt_txt_heap;
diff --git a/src/test/regress/expected/constraints.out b/src/test/regress/expected/constraints.out
index e6f6602d953..d839678e6c7 100644
--- a/src/test/regress/expected/constraints.out
+++ b/src/test/regress/expected/constraints.out
@@ -349,7 +349,8 @@ CREATE TABLE COPY_TBL (x INT, y TEXT, z INT,
 	CONSTRAINT COPY_CON
 	CHECK (x > 3 AND y <> 'check failed' AND x < 7 ));
 \set filename :abs_srcdir '/data/constro.data'
-COPY COPY_TBL FROM :'filename';
+\set command '\\copy COPY_TBL FROM ' :'filename';
+:command
 SELECT * FROM COPY_TBL;
  x |       y       | z 
 ---+---------------+---
@@ -358,7 +359,8 @@ SELECT * FROM COPY_TBL;
 (2 rows)
 
 \set filename :abs_srcdir '/data/constrf.data'
-COPY COPY_TBL FROM :'filename';
+\set command '\\copy COPY_TBL FROM ' :'filename';
+:command
 ERROR:  new row for relation "copy_tbl" violates check constraint "copy_con"
 DETAIL:  Failing row contains (7, check failed, 6).
 CONTEXT:  COPY copy_tbl, line 2: "7	check failed	6"
diff --git a/src/test/regress/expected/copy.out b/src/test/regress/expected/copy.out
index b48365ec981..13bee7a5ee4 100644
--- a/src/test/regress/expected/copy.out
+++ b/src/test/regress/expected/copy.out
@@ -15,9 +15,11 @@ insert into copytest values('Unix',E'abc\ndef',2);
 insert into copytest values('Mac',E'abc\rdef',3);
 insert into copytest values(E'esc\\ape',E'a\\r\\\r\\\n\\nb',4);
 \set filename :abs_builddir '/results/copytest.csv'
-copy copytest to :'filename' csv;
+\set command '\\copy copytest to ' :'filename' csv;
+:command
 create temp table copytest2 (like copytest);
-copy copytest2 from :'filename' csv;
+\set command '\\copy copytest2 from ' :'filename' csv;
+:command
 select * from copytest except select * from copytest2;
  style | test | filler 
 -------+------+--------
@@ -25,8 +27,10 @@ select * from copytest except select * from copytest2;
 
 truncate copytest2;
 --- same test but with an escape char different from quote char
-copy copytest to :'filename' csv quote '''' escape E'\\';
-copy copytest2 from :'filename' csv quote '''' escape E'\\';
+\set command '\\copy copytest to ' :'filename' ' csv quote ' '\'\'\'\'' ' escape ' 'E\'' '\\\\\'';
+:command
+\set command '\\copy copytest2 from ' :'filename' ' csv quote ' '\'\'\'\'' ' escape ' 'E\'' '\\\\\'';
+:command
 select * from copytest except select * from copytest2;
  style | test | filler 
 -------+------+--------
@@ -66,13 +70,16 @@ insert into parted_copytest select x,1,'One' from generate_series(1,1000) x;
 insert into parted_copytest select x,2,'Two' from generate_series(1001,1010) x;
 insert into parted_copytest select x,1,'One' from generate_series(1011,1020) x;
 \set filename :abs_builddir '/results/parted_copytest.csv'
-copy (select * from parted_copytest order by a) to :'filename';
+\set command '\\copy (select * from parted_copytest order by a) to ' :'filename';
+:command
 truncate parted_copytest;
-copy parted_copytest from :'filename';
+\set command '\\copy parted_copytest from ' :'filename';
+:command
 -- Ensure COPY FREEZE errors for partitioned tables.
 begin;
 truncate parted_copytest;
-copy parted_copytest from :'filename' (freeze);
+\set command '\\copy parted_copytest from ' :'filename' (freeze);
+:command
 ERROR:  cannot perform COPY FREEZE on a partitioned table
 rollback;
 select tableoid::regclass,count(*),sum(a) from parted_copytest
@@ -94,7 +101,8 @@ create trigger part_ins_trig
 	before insert on parted_copytest_a2
 	for each row
 	execute procedure part_ins_func();
-copy parted_copytest from :'filename';
+\set command '\\copy parted_copytest from ' :'filename';
+:command
 select tableoid::regclass,count(*),sum(a) from parted_copytest
 group by tableoid order by tableoid::regclass::name;
       tableoid      | count |  sum   
@@ -106,7 +114,8 @@ group by tableoid order by tableoid::regclass::name;
 truncate table parted_copytest;
 create index on parted_copytest (b);
 drop trigger part_ins_trig on parted_copytest_a2;
-copy parted_copytest from stdin;
+\set command '\\copy parted_copytest from ' stdin;
+:command
 -- Ensure index entries were properly added during the copy.
 select * from parted_copytest where b = 1;
  a | b |  c   
@@ -170,9 +179,10 @@ INFO:  progress: {"type": "PIPE", "command": "COPY FROM", "relname": "tab_progre
 -- Generate COPY FROM report with FILE, with some excluded tuples.
 truncate tab_progress_reporting;
 \set filename :abs_srcdir '/data/emp.data'
-copy tab_progress_reporting from :'filename'
-	where (salary < 2000);
-INFO:  progress: {"type": "FILE", "command": "COPY FROM", "relname": "tab_progress_reporting", "has_bytes_total": true, "tuples_excluded": 1, "tuples_processed": 2, "has_bytes_processed": true}
+\set command '\\copy tab_progress_reporting from ' :'filename' 'where (salary < 2000)';
+:command
+INFO:  progress: {"type": "PIPE", "command": "COPY FROM", "relname": "tab_progress_reporting", "has_bytes_total": false, "tuples_excluded": 1, "tuples_processed": 2, "has_bytes_processed": true}
+	
 drop trigger check_after_tab_progress_reporting on tab_progress_reporting;
 drop function notice_after_tab_progress_reporting();
 drop table tab_progress_reporting;
@@ -281,7 +291,8 @@ CREATE TABLE parted_si_p_odd PARTITION OF parted_si FOR VALUES IN (1);
 -- https://postgr.es/m/18130-7a86a7356a75209d%40postgresql.org
 -- https://postgr.es/m/257696.1695670946%40sss.pgh.pa.us
 \set filename :abs_srcdir '/data/desc.data'
-COPY parted_si(id, data) FROM :'filename';
+\set command '\\COPY parted_si(id, data) FROM ' :'filename';
+:command
 -- An earlier bug (see commit b1ecb9b3fcf) could end up using a buffer from
 -- the wrong partition. This test is *not* guaranteed to trigger that bug, but
 -- does so when shared_buffers is small enough.  To test if we encountered the
diff --git a/src/test/regress/expected/create_index.out b/src/test/regress/expected/create_index.out
index acfd9d1f4f7..7a230026ad7 100644
--- a/src/test/regress/expected/create_index.out
+++ b/src/test/regress/expected/create_index.out
@@ -51,7 +51,8 @@ CREATE TABLE fast_emp4000 (
 	home_base	 box
 );
 \set filename :abs_srcdir '/data/rect.data'
-COPY slow_emp4000 FROM :'filename';
+\set command '\\copy slow_emp4000 FROM ' :'filename';
+:command
 INSERT INTO fast_emp4000 SELECT * FROM slow_emp4000;
 ANALYZE slow_emp4000;
 ANALYZE fast_emp4000;
@@ -655,7 +656,8 @@ CREATE TABLE array_index_op_test (
 	t			text[]
 );
 \set filename :abs_srcdir '/data/array.data'
-COPY array_index_op_test FROM :'filename';
+\set command '\\copy array_index_op_test FROM ' :'filename';
+:command
 ANALYZE array_index_op_test;
 SELECT * FROM array_index_op_test WHERE i = '{NULL}' ORDER BY seqno;
  seqno |   i    |   t    
diff --git a/src/test/regress/expected/create_view.out b/src/test/regress/expected/create_view.out
index 61825ef7d4e..a2bff59d704 100644
--- a/src/test/regress/expected/create_view.out
+++ b/src/test/regress/expected/create_view.out
@@ -18,7 +18,8 @@ CREATE TABLE real_city (
 	outline 	path
 );
 \set filename :abs_srcdir '/data/real_city.data'
-COPY real_city FROM :'filename';
+\set command '\\copy real_city FROM ' :'filename';
+:command
 ANALYZE real_city;
 SELECT *
    INTO TABLE ramp
diff --git a/src/test/regress/expected/hash_index.out b/src/test/regress/expected/hash_index.out
index a2036a15972..805d73b9d2e 100644
--- a/src/test/regress/expected/hash_index.out
+++ b/src/test/regress/expected/hash_index.out
@@ -20,10 +20,14 @@ CREATE TABLE hash_f8_heap (
 	random 		float8
 );
 \set filename :abs_srcdir '/data/hash.data'
-COPY hash_i4_heap FROM :'filename';
-COPY hash_name_heap FROM :'filename';
-COPY hash_txt_heap FROM :'filename';
-COPY hash_f8_heap FROM :'filename';
+\set command '\\copy hash_i4_heap FROM ' :'filename';
+:command
+\set command '\\copy hash_name_heap FROM ' :'filename';
+:command
+\set command '\\copy hash_txt_heap FROM ' :'filename';
+:command
+\set command '\\copy hash_f8_heap FROM ' :'filename';
+:command
 -- the data in this file has a lot of duplicates in the index key
 -- fields, leading to long bucket chains and lots of table expansion.
 -- this is therefore a stress test of the bucket overflow code (unlike
diff --git a/src/test/regress/expected/jsonb.out b/src/test/regress/expected/jsonb.out
index f8a7dac9607..64dcaf171c5 100644
--- a/src/test/regress/expected/jsonb.out
+++ b/src/test/regress/expected/jsonb.out
@@ -4,7 +4,8 @@ CREATE TABLE testjsonb (
        j jsonb
 );
 \set filename :abs_srcdir '/data/jsonb.data'
-COPY testjsonb FROM :'filename';
+\set command '\\copy testjsonb FROM ' :'filename';
+:command
 -- Strings.
 SELECT '""'::jsonb;				-- OK.
  jsonb 
diff --git a/src/test/regress/expected/largeobject.out b/src/test/regress/expected/largeobject.out
index 4921dd79aee..8732c2cb67a 100644
--- a/src/test/regress/expected/largeobject.out
+++ b/src/test/regress/expected/largeobject.out
@@ -346,7 +346,8 @@ SELECT lo_unlink(loid) from lotest_stash_values;
 
 TRUNCATE lotest_stash_values;
 \set filename :abs_srcdir '/data/tenk.data'
-INSERT INTO lotest_stash_values (loid) SELECT lo_import(:'filename');
+\lo_import :filename
+INSERT INTO lotest_stash_values (loid) VALUES (:LASTOID);
 BEGIN;
 UPDATE lotest_stash_values SET fd=lo_open(loid, CAST(x'20000' | x'40000' AS integer));
 -- verify length of large object
@@ -410,12 +411,8 @@ SELECT lo_close(fd) FROM lotest_stash_values;
 
 END;
 \set filename :abs_builddir '/results/lotest.txt'
-SELECT lo_export(loid, :'filename') FROM lotest_stash_values;
- lo_export 
------------
-         1
-(1 row)
-
+SELECT loid FROM lotest_stash_values \gset
+\lo_export :loid, :filename
 \lo_import :filename
 \set newloid :LASTOID
 -- just make sure \lo_export does not barf
diff --git a/src/test/regress/expected/misc.out b/src/test/regress/expected/misc.out
index 6e816c57f1f..6ef45b468e9 100644
--- a/src/test/regress/expected/misc.out
+++ b/src/test/regress/expected/misc.out
@@ -59,9 +59,11 @@ DROP TABLE tmp;
 -- copy
 --
 \set filename :abs_builddir '/results/onek.data'
-COPY onek TO :'filename';
+\set command '\\copy onek TO ' :'filename';
+:command
 CREATE TEMP TABLE onek_copy (LIKE onek);
-COPY onek_copy FROM :'filename';
+\set command '\\copy onek_copy FROM ' :'filename';
+:command
 SELECT * FROM onek EXCEPT ALL SELECT * FROM onek_copy;
  unique1 | unique2 | two | four | ten | twenty | hundred | thousand | twothousand | fivethous | tenthous | odd | even | stringu1 | stringu2 | string4 
 ---------+---------+-----+------+-----+--------+---------+----------+-------------+-----------+----------+-----+------+----------+----------+---------
@@ -73,9 +75,11 @@ SELECT * FROM onek_copy EXCEPT ALL SELECT * FROM onek;
 (0 rows)
 
 \set filename :abs_builddir '/results/stud_emp.data'
-COPY BINARY stud_emp TO :'filename';
+\set command '\\COPY BINARY stud_emp TO ' :'filename';
+:command
 CREATE TEMP TABLE stud_emp_copy (LIKE stud_emp);
-COPY BINARY stud_emp_copy FROM :'filename';
+\set command '\\COPY BINARY stud_emp_copy FROM ' :'filename';
+:command
 SELECT * FROM stud_emp_copy;
  name  | age |  location  | salary | manager | gpa | percent 
 -------+-----+------------+--------+---------+-----+---------
diff --git a/src/test/regress/expected/password.out b/src/test/regress/expected/password.out
index 84752317356..3d678d232b8 100644
--- a/src/test/regress/expected/password.out
+++ b/src/test/regress/expected/password.out
@@ -12,11 +12,11 @@ SET password_encryption = 'md5'; -- ok
 SET password_encryption = 'scram-sha-256'; -- ok
 -- consistency of password entries
 SET password_encryption = 'md5';
-CREATE ROLE regress_passwd1 PASSWORD 'role_pwd1';
-CREATE ROLE regress_passwd2 PASSWORD 'role_pwd2';
+CREATE ROLE regress_passwd1 PASSWORD 'role_pwd1Mahqu4eit2';
+CREATE ROLE regress_passwd2 PASSWORD 'role_pwd2Mahqu4eit2';
 SET password_encryption = 'scram-sha-256';
-CREATE ROLE regress_passwd3 PASSWORD 'role_pwd3';
-CREATE ROLE regress_passwd4 PASSWORD NULL;
+CREATE ROLE regress_passwd3 PASSWORD 'role_pwd3Mahqu4eit2';
+CREATE ROLE regress_passwd4 PASSWORD 'Mahqu4eit2FHS';
 -- check list of created entries
 --
 -- The scram secret will look something like:
@@ -30,10 +30,10 @@ SELECT rolname, regexp_replace(rolpassword, '(SCRAM-SHA-256)\$(\d+):([a-zA-Z0-9+
     ORDER BY rolname, rolpassword;
      rolname     |                rolpassword_masked                 
 -----------------+---------------------------------------------------
- regress_passwd1 | md5783277baca28003b33453252be4dbb34
- regress_passwd2 | md54044304ba511dd062133eb5b4b84a2a3
+ regress_passwd1 | md5cf164b1eb55cd11cd73be38189a421f6
+ regress_passwd2 | md51b973138181bae7549ef288d52425a2e
  regress_passwd3 | SCRAM-SHA-256$4096:<salt>$<storedkey>:<serverkey>
- regress_passwd4 | 
+ regress_passwd4 | SCRAM-SHA-256$4096:<salt>$<storedkey>:<serverkey>
 (4 rows)
 
 -- Rename a role
@@ -54,24 +54,30 @@ ALTER ROLE regress_passwd2_new RENAME TO regress_passwd2;
 -- passwords.
 SET password_encryption = 'md5';
 -- encrypt with MD5
-ALTER ROLE regress_passwd2 PASSWORD 'foo';
+ALTER ROLE regress_passwd2 PASSWORD 'Mahqu4eit2FOO';
 -- already encrypted, use as they are
 ALTER ROLE regress_passwd1 PASSWORD 'md5cd3578025fe2c3d7ed1b9a9b26238b70';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 ALTER ROLE regress_passwd3 PASSWORD 'SCRAM-SHA-256$4096:VLK4RMaQLCvNtQ==$6YtlR4t69SguDiwFvbVgVZtuz6gpJQQqUMZ7IQJK5yI=:ps75jrHeYU4lXCcXI4O8oIdJ3eO8o2jirjruw9phBTo=';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 SET password_encryption = 'scram-sha-256';
 -- create SCRAM secret
-ALTER ROLE  regress_passwd4 PASSWORD 'foo';
+ALTER ROLE  regress_passwd4 PASSWORD 'Mahqu4eit2foo';
 -- already encrypted with MD5, use as it is
 CREATE ROLE regress_passwd5 PASSWORD 'md5e73a4b11df52a6068f8b39f90be36023';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 -- This looks like a valid SCRAM-SHA-256 secret, but it is not
 -- so it should be hashed with SCRAM-SHA-256.
 CREATE ROLE regress_passwd6 PASSWORD 'SCRAM-SHA-256$1234';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 -- These may look like valid MD5 secrets, but they are not, so they
 -- should be hashed with SCRAM-SHA-256.
 -- trailing garbage at the end
 CREATE ROLE regress_passwd7 PASSWORD 'md5012345678901234567890123456789zz';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 -- invalid length
 CREATE ROLE regress_passwd8 PASSWORD 'md501234567890123456789012345678901zz';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 -- Changing the SCRAM iteration count
 SET scram_iterations = 1024;
 CREATE ROLE regress_passwd9 PASSWORD 'alterediterationcount';
@@ -81,63 +87,67 @@ SELECT rolname, regexp_replace(rolpassword, '(SCRAM-SHA-256)\$(\d+):([a-zA-Z0-9+
     ORDER BY rolname, rolpassword;
      rolname     |                rolpassword_masked                 
 -----------------+---------------------------------------------------
- regress_passwd1 | md5cd3578025fe2c3d7ed1b9a9b26238b70
- regress_passwd2 | md5dfa155cadd5f4ad57860162f3fab9cdb
+ regress_passwd1 | md5cf164b1eb55cd11cd73be38189a421f6
+ regress_passwd2 | md51c72b7907ee13c2116fd36cc255ed306
  regress_passwd3 | SCRAM-SHA-256$4096:<salt>$<storedkey>:<serverkey>
  regress_passwd4 | SCRAM-SHA-256$4096:<salt>$<storedkey>:<serverkey>
- regress_passwd5 | md5e73a4b11df52a6068f8b39f90be36023
- regress_passwd6 | SCRAM-SHA-256$4096:<salt>$<storedkey>:<serverkey>
- regress_passwd7 | SCRAM-SHA-256$4096:<salt>$<storedkey>:<serverkey>
- regress_passwd8 | SCRAM-SHA-256$4096:<salt>$<storedkey>:<serverkey>
  regress_passwd9 | SCRAM-SHA-256$1024:<salt>$<storedkey>:<serverkey>
-(9 rows)
+(5 rows)
 
 -- An empty password is not allowed, in any form
 CREATE ROLE regress_passwd_empty PASSWORD '';
 NOTICE:  empty string is not a valid password, clearing password
+ERROR:  Failed to get encrypted password: User "regress_passwd_empty" has no password assigned.
 ALTER ROLE regress_passwd_empty PASSWORD 'md585939a5ce845f1a1b620742e3c659e0a';
-NOTICE:  empty string is not a valid password, clearing password
+ERROR:  role "regress_passwd_empty" does not exist
 ALTER ROLE regress_passwd_empty PASSWORD 'SCRAM-SHA-256$4096:hpFyHTUsSWcR7O9P$LgZFIt6Oqdo27ZFKbZ2nV+vtnYM995pDh9ca6WSi120=:qVV5NeluNfUPkwm7Vqat25RjSPLkGeoZBQs6wVv+um4=';
-NOTICE:  empty string is not a valid password, clearing password
+ERROR:  role "regress_passwd_empty" does not exist
 SELECT rolpassword FROM pg_authid WHERE rolname='regress_passwd_empty';
  rolpassword 
 -------------
- 
-(1 row)
+(0 rows)
 
 -- Test with invalid stored and server keys.
 --
 -- The first is valid, to act as a control. The others have too long
 -- stored/server keys. They will be re-hashed.
 CREATE ROLE regress_passwd_sha_len0 PASSWORD 'SCRAM-SHA-256$4096:A6xHKoH/494E941doaPOYg==$Ky+A30sewHIH3VHQLRN9vYsuzlgNyGNKCh37dy96Rqw=:COPdlNiIkrsacU5QoxydEuOH6e/KfiipeETb/bPw8ZI=';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 CREATE ROLE regress_passwd_sha_len1 PASSWORD 'SCRAM-SHA-256$4096:A6xHKoH/494E941doaPOYg==$Ky+A30sewHIH3VHQLRN9vYsuzlgNyGNKCh37dy96RqwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=:COPdlNiIkrsacU5QoxydEuOH6e/KfiipeETb/bPw8ZI=';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 CREATE ROLE regress_passwd_sha_len2 PASSWORD 'SCRAM-SHA-256$4096:A6xHKoH/494E941doaPOYg==$Ky+A30sewHIH3VHQLRN9vYsuzlgNyGNKCh37dy96Rqw=:COPdlNiIkrsacU5QoxydEuOH6e/KfiipeETb/bPw8ZIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=';
+ERROR:  Received HTTP code 400 from control plane: {"error":"Neon only supports being given plaintext passwords"}
 -- Check that the invalid secrets were re-hashed. A re-hashed secret
 -- should not contain the original salt.
 SELECT rolname, rolpassword not like '%A6xHKoH/494E941doaPOYg==%' as is_rolpassword_rehashed
     FROM pg_authid
     WHERE rolname LIKE 'regress_passwd_sha_len%'
     ORDER BY rolname;
-         rolname         | is_rolpassword_rehashed 
--------------------------+-------------------------
- regress_passwd_sha_len0 | f
- regress_passwd_sha_len1 | t
- regress_passwd_sha_len2 | t
-(3 rows)
+ rolname | is_rolpassword_rehashed 
+---------+-------------------------
+(0 rows)
 
 DROP ROLE regress_passwd1;
 DROP ROLE regress_passwd2;
 DROP ROLE regress_passwd3;
 DROP ROLE regress_passwd4;
 DROP ROLE regress_passwd5;
+ERROR:  role "regress_passwd5" does not exist
 DROP ROLE regress_passwd6;
+ERROR:  role "regress_passwd6" does not exist
 DROP ROLE regress_passwd7;
+ERROR:  role "regress_passwd7" does not exist
 DROP ROLE regress_passwd8;
+ERROR:  role "regress_passwd8" does not exist
 DROP ROLE regress_passwd9;
 DROP ROLE regress_passwd_empty;
+ERROR:  role "regress_passwd_empty" does not exist
 DROP ROLE regress_passwd_sha_len0;
+ERROR:  role "regress_passwd_sha_len0" does not exist
 DROP ROLE regress_passwd_sha_len1;
+ERROR:  role "regress_passwd_sha_len1" does not exist
 DROP ROLE regress_passwd_sha_len2;
+ERROR:  role "regress_passwd_sha_len2" does not exist
 -- all entries should have been removed
 SELECT rolname, rolpassword
     FROM pg_authid
diff --git a/src/test/regress/expected/psql.out b/src/test/regress/expected/psql.out
index 7cd0c27cca8..e85353d2aa1 100644
--- a/src/test/regress/expected/psql.out
+++ b/src/test/regress/expected/psql.out
@@ -5544,11 +5544,14 @@ CREATE TEMPORARY TABLE reload_output(
   line text
 );
 SELECT 1 AS a \g :g_out_file
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 SELECT 2 AS b\; SELECT 3 AS c\; SELECT 4 AS d \g :g_out_file
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 COPY (SELECT 'foo') TO STDOUT \; COPY (SELECT 'bar') TO STDOUT \g :g_out_file
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
   line   
 ---------
@@ -5587,13 +5590,15 @@ SELECT 1 AS a\; SELECT 2 AS b\; SELECT 3 AS c;
 -- COPY TO file
 -- The data goes to :g_out_file and the status to :o_out_file
 \set QUIET false
-COPY (SELECT unique1 FROM onek ORDER BY unique1 LIMIT 10) TO :'g_out_file';
+\set command '\\COPY (SELECT unique1 FROM onek ORDER BY unique1 LIMIT 10) TO ' :'g_out_file';
+:command 
 -- DML command status
 UPDATE onek SET unique1 = unique1 WHERE false;
 \set QUIET true
 \o
 -- Check the contents of the files generated.
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
  line 
 ------
@@ -5610,7 +5615,8 @@ SELECT line FROM reload_output ORDER BY lineno;
 (10 rows)
 
 TRUNCATE TABLE reload_output;
-COPY reload_output(line) FROM :'o_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'o_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
    line   
 ----------
@@ -5647,7 +5653,8 @@ COPY (SELECT 'foo1') TO STDOUT \; COPY (SELECT 'bar1') TO STDOUT;
 COPY (SELECT 'foo2') TO STDOUT \; COPY (SELECT 'bar2') TO STDOUT \g :g_out_file
 \o
 -- Check the contents of the files generated.
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
  line 
 ------
@@ -5656,7 +5663,8 @@ SELECT line FROM reload_output ORDER BY lineno;
 (2 rows)
 
 TRUNCATE TABLE reload_output;
-COPY reload_output(line) FROM :'o_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'o_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
  line 
 ------
diff --git a/src/test/regress/expected/security_label.out b/src/test/regress/expected/security_label.out
index a8e01a6220a..10c500d6a7d 100644
--- a/src/test/regress/expected/security_label.out
+++ b/src/test/regress/expected/security_label.out
@@ -19,21 +19,21 @@ ALTER TABLE seclabel_tbl2 OWNER TO regress_seclabel_user2;
 -- Test of SECURITY LABEL statement without a plugin
 --
 SECURITY LABEL ON TABLE seclabel_tbl1 IS 'classified';			-- fail
-ERROR:  no security label providers have been loaded
+ERROR:  must specify provider when multiple security label providers have been loaded
 SECURITY LABEL FOR 'dummy' ON TABLE seclabel_tbl1 IS 'classified';		-- fail
 ERROR:  security label provider "dummy" is not loaded
 SECURITY LABEL ON TABLE seclabel_tbl1 IS '...invalid label...';		-- fail
-ERROR:  no security label providers have been loaded
+ERROR:  must specify provider when multiple security label providers have been loaded
 SECURITY LABEL ON TABLE seclabel_tbl3 IS 'unclassified';			-- fail
-ERROR:  no security label providers have been loaded
+ERROR:  must specify provider when multiple security label providers have been loaded
 SECURITY LABEL ON ROLE regress_seclabel_user1 IS 'classified';			-- fail
-ERROR:  no security label providers have been loaded
+ERROR:  must specify provider when multiple security label providers have been loaded
 SECURITY LABEL FOR 'dummy' ON ROLE regress_seclabel_user1 IS 'classified';		-- fail
 ERROR:  security label provider "dummy" is not loaded
 SECURITY LABEL ON ROLE regress_seclabel_user1 IS '...invalid label...';		-- fail
-ERROR:  no security label providers have been loaded
+ERROR:  must specify provider when multiple security label providers have been loaded
 SECURITY LABEL ON ROLE regress_seclabel_user3 IS 'unclassified';			-- fail
-ERROR:  no security label providers have been loaded
+ERROR:  must specify provider when multiple security label providers have been loaded
 -- clean up objects
 DROP FUNCTION seclabel_four();
 DROP DOMAIN seclabel_domain;
diff --git a/src/test/regress/expected/stats.out b/src/test/regress/expected/stats.out
index 94187e59cfb..97189594579 100644
--- a/src/test/regress/expected/stats.out
+++ b/src/test/regress/expected/stats.out
@@ -1291,14 +1291,11 @@ SELECT sum(reads) AS io_sum_shared_before_reads
 -- rewritten table, e.g. by autovacuum.
 BEGIN;
 ALTER TABLE test_io_shared SET TABLESPACE regress_tblspace;
+ERROR:  tablespace "regress_tblspace" does not exist
 -- SELECT from the table so that the data is read into shared buffers and
 -- context 'normal', object 'relation' reads are counted.
 SELECT COUNT(*) FROM test_io_shared;
- count 
--------
-   100
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 SELECT pg_stat_force_next_flush();
  pg_stat_force_next_flush 
@@ -1415,6 +1412,7 @@ SELECT :io_sum_local_after_evictions > :io_sum_local_before_evictions,
 -- local buffers, exercising a different codepath than standard local buffer
 -- writes.
 ALTER TABLE test_io_local SET TABLESPACE regress_tblspace;
+ERROR:  tablespace "regress_tblspace" does not exist
 SELECT pg_stat_force_next_flush();
  pg_stat_force_next_flush 
 --------------------------
@@ -1426,7 +1424,7 @@ SELECT sum(writes) AS io_sum_local_new_tblspc_writes
 SELECT :io_sum_local_new_tblspc_writes > :io_sum_local_after_writes;
  ?column? 
 ----------
- t
+ f
 (1 row)
 
 RESET temp_buffers;
diff --git a/src/test/regress/expected/test_setup.out b/src/test/regress/expected/test_setup.out
index 5d9e6bf12bc..c5fddfdca65 100644
--- a/src/test/regress/expected/test_setup.out
+++ b/src/test/regress/expected/test_setup.out
@@ -21,6 +21,7 @@ GRANT ALL ON SCHEMA public TO public;
 -- Create a tablespace we can use in tests.
 SET allow_in_place_tablespaces = true;
 CREATE TABLESPACE regress_tblspace LOCATION '';
+ERROR:  CREATE TABLESPACE is not supported on Neon
 --
 -- These tables have traditionally been referenced by many tests,
 -- so create and populate them.  Insert only non-error values here.
@@ -111,7 +112,8 @@ CREATE TABLE onek (
 	string4		name
 );
 \set filename :abs_srcdir '/data/onek.data'
-COPY onek FROM :'filename';
+\set command '\\copy onek FROM ' :'filename';
+:command
 VACUUM ANALYZE onek;
 CREATE TABLE onek2 AS SELECT * FROM onek;
 VACUUM ANALYZE onek2;
@@ -134,7 +136,8 @@ CREATE TABLE tenk1 (
 	string4		name
 );
 \set filename :abs_srcdir '/data/tenk.data'
-COPY tenk1 FROM :'filename';
+\set command '\\copy tenk1 FROM ' :'filename';
+:command
 VACUUM ANALYZE tenk1;
 CREATE TABLE tenk2 AS SELECT * FROM tenk1;
 VACUUM ANALYZE tenk2;
@@ -144,20 +147,23 @@ CREATE TABLE person (
 	location 	point
 );
 \set filename :abs_srcdir '/data/person.data'
-COPY person FROM :'filename';
+\set command '\\copy person FROM ' :'filename';
+:command
 VACUUM ANALYZE person;
 CREATE TABLE emp (
 	salary 		int4,
 	manager 	name
 ) INHERITS (person);
 \set filename :abs_srcdir '/data/emp.data'
-COPY emp FROM :'filename';
+\set command '\\copy emp FROM ' :'filename';
+:command
 VACUUM ANALYZE emp;
 CREATE TABLE student (
 	gpa 		float8
 ) INHERITS (person);
 \set filename :abs_srcdir '/data/student.data'
-COPY student FROM :'filename';
+\set command '\\copy student FROM ' :'filename';
+:command
 VACUUM ANALYZE student;
 CREATE TABLE stud_emp (
 	percent 	int4
@@ -166,14 +172,16 @@ NOTICE:  merging multiple inherited definitions of column "name"
 NOTICE:  merging multiple inherited definitions of column "age"
 NOTICE:  merging multiple inherited definitions of column "location"
 \set filename :abs_srcdir '/data/stud_emp.data'
-COPY stud_emp FROM :'filename';
+\set command '\\copy stud_emp FROM ' :'filename';
+:command
 VACUUM ANALYZE stud_emp;
 CREATE TABLE road (
 	name		text,
 	thepath 	path
 );
 \set filename :abs_srcdir '/data/streets.data'
-COPY road FROM :'filename';
+\set command '\\copy road FROM ' :'filename';
+:command
 VACUUM ANALYZE road;
 CREATE TABLE ihighway () INHERITS (road);
 INSERT INTO ihighway
diff --git a/src/test/regress/expected/tsearch.out b/src/test/regress/expected/tsearch.out
index cfa391ac512..73853735818 100644
--- a/src/test/regress/expected/tsearch.out
+++ b/src/test/regress/expected/tsearch.out
@@ -63,7 +63,8 @@ CREATE TABLE test_tsvector(
 	a tsvector
 );
 \set filename :abs_srcdir '/data/tsearch.data'
-COPY test_tsvector FROM :'filename';
+\set command '\\copy test_tsvector FROM ' :'filename';
+:command
 ANALYZE test_tsvector;
 -- test basic text search behavior without indexes, then with
 SELECT count(*) FROM test_tsvector WHERE a @@ 'wr|qh';
diff --git a/src/test/regress/parallel_schedule b/src/test/regress/parallel_schedule
index cf46fa33593..709602df283 100644
--- a/src/test/regress/parallel_schedule
+++ b/src/test/regress/parallel_schedule
@@ -130,4 +130,4 @@ test: fast_default
 
 # run tablespace test at the end because it drops the tablespace created during
 # setup that other tests may use.
-test: tablespace
+#test: tablespace
diff --git a/src/test/regress/sql/aggregates.sql b/src/test/regress/sql/aggregates.sql
index 758ad906516..1ea43f1e2df 100644
--- a/src/test/regress/sql/aggregates.sql
+++ b/src/test/regress/sql/aggregates.sql
@@ -15,7 +15,8 @@ CREATE TABLE aggtest (
 );
 
 \set filename :abs_srcdir '/data/agg.data'
-COPY aggtest FROM :'filename';
+\set command '\\copy aggtest FROM ' :'filename';
+:command
 
 ANALYZE aggtest;
 
diff --git a/src/test/regress/sql/arrays.sql b/src/test/regress/sql/arrays.sql
index daf805c3827..35abfb841c7 100644
--- a/src/test/regress/sql/arrays.sql
+++ b/src/test/regress/sql/arrays.sql
@@ -22,7 +22,8 @@ CREATE TABLE array_op_test (
 );
 
 \set filename :abs_srcdir '/data/array.data'
-COPY array_op_test FROM :'filename';
+\set command '\\copy array_op_test FROM ' :'filename';
+:command
 ANALYZE array_op_test;
 
 --
diff --git a/src/test/regress/sql/btree_index.sql b/src/test/regress/sql/btree_index.sql
index 239f4a4755f..f29d87bdff7 100644
--- a/src/test/regress/sql/btree_index.sql
+++ b/src/test/regress/sql/btree_index.sql
@@ -26,16 +26,20 @@ CREATE TABLE bt_f8_heap (
 );
 
 \set filename :abs_srcdir '/data/desc.data'
-COPY bt_i4_heap FROM :'filename';
+\set command '\\copy bt_i4_heap FROM ' :'filename';
+:command
 
 \set filename :abs_srcdir '/data/hash.data'
-COPY bt_name_heap FROM :'filename';
+\set command '\\copy bt_name_heap FROM ' :'filename';
+:command
 
 \set filename :abs_srcdir '/data/desc.data'
-COPY bt_txt_heap FROM :'filename';
+\set command '\\copy bt_txt_heap FROM ' :'filename';
+:command
 
 \set filename :abs_srcdir '/data/hash.data'
-COPY bt_f8_heap FROM :'filename';
+\set command '\\copy bt_f8_heap FROM ' :'filename';
+:command
 
 ANALYZE bt_i4_heap;
 ANALYZE bt_name_heap;
diff --git a/src/test/regress/sql/constraints.sql b/src/test/regress/sql/constraints.sql
index 5ffcd4ffc7b..913c7ad80d1 100644
--- a/src/test/regress/sql/constraints.sql
+++ b/src/test/regress/sql/constraints.sql
@@ -243,12 +243,14 @@ CREATE TABLE COPY_TBL (x INT, y TEXT, z INT,
 	CHECK (x > 3 AND y <> 'check failed' AND x < 7 ));
 
 \set filename :abs_srcdir '/data/constro.data'
-COPY COPY_TBL FROM :'filename';
+\set command '\\copy COPY_TBL FROM ' :'filename';
+:command
 
 SELECT * FROM COPY_TBL;
 
 \set filename :abs_srcdir '/data/constrf.data'
-COPY COPY_TBL FROM :'filename';
+\set command '\\copy COPY_TBL FROM ' :'filename';
+:command
 
 SELECT * FROM COPY_TBL;
 
diff --git a/src/test/regress/sql/copy.sql b/src/test/regress/sql/copy.sql
index 43d2e906dd9..6c993d70f05 100644
--- a/src/test/regress/sql/copy.sql
+++ b/src/test/regress/sql/copy.sql
@@ -20,11 +20,13 @@ insert into copytest values('Mac',E'abc\rdef',3);
 insert into copytest values(E'esc\\ape',E'a\\r\\\r\\\n\\nb',4);
 
 \set filename :abs_builddir '/results/copytest.csv'
-copy copytest to :'filename' csv;
+\set command '\\copy copytest to ' :'filename' csv;
+:command
 
 create temp table copytest2 (like copytest);
 
-copy copytest2 from :'filename' csv;
+\set command '\\copy copytest2 from ' :'filename' csv;
+:command
 
 select * from copytest except select * from copytest2;
 
@@ -32,9 +34,11 @@ truncate copytest2;
 
 --- same test but with an escape char different from quote char
 
-copy copytest to :'filename' csv quote '''' escape E'\\';
+\set command '\\copy copytest to ' :'filename' ' csv quote ' '\'\'\'\'' ' escape ' 'E\'' '\\\\\'';
+:command
 
-copy copytest2 from :'filename' csv quote '''' escape E'\\';
+\set command '\\copy copytest2 from ' :'filename' ' csv quote ' '\'\'\'\'' ' escape ' 'E\'' '\\\\\'';
+:command
 
 select * from copytest except select * from copytest2;
 
@@ -86,16 +90,19 @@ insert into parted_copytest select x,2,'Two' from generate_series(1001,1010) x;
 insert into parted_copytest select x,1,'One' from generate_series(1011,1020) x;
 
 \set filename :abs_builddir '/results/parted_copytest.csv'
-copy (select * from parted_copytest order by a) to :'filename';
+\set command '\\copy (select * from parted_copytest order by a) to ' :'filename';
+:command
 
 truncate parted_copytest;
 
-copy parted_copytest from :'filename';
+\set command '\\copy parted_copytest from ' :'filename';
+:command
 
 -- Ensure COPY FREEZE errors for partitioned tables.
 begin;
 truncate parted_copytest;
-copy parted_copytest from :'filename' (freeze);
+\set command '\\copy parted_copytest from ' :'filename' (freeze);
+:command
 rollback;
 
 select tableoid::regclass,count(*),sum(a) from parted_copytest
@@ -115,7 +122,8 @@ create trigger part_ins_trig
 	for each row
 	execute procedure part_ins_func();
 
-copy parted_copytest from :'filename';
+\set command '\\copy parted_copytest from ' :'filename';
+:command
 
 select tableoid::regclass,count(*),sum(a) from parted_copytest
 group by tableoid order by tableoid::regclass::name;
@@ -124,7 +132,8 @@ truncate table parted_copytest;
 create index on parted_copytest (b);
 drop trigger part_ins_trig on parted_copytest_a2;
 
-copy parted_copytest from stdin;
+\set command '\\copy parted_copytest from ' stdin;
+:command
 1	1	str1
 2	2	str2
 \.
@@ -191,8 +200,8 @@ bill	20	(11,10)	1000	sharon
 -- Generate COPY FROM report with FILE, with some excluded tuples.
 truncate tab_progress_reporting;
 \set filename :abs_srcdir '/data/emp.data'
-copy tab_progress_reporting from :'filename'
-	where (salary < 2000);
+\set command '\\copy tab_progress_reporting from ' :'filename' 'where (salary < 2000)';
+:command
 
 drop trigger check_after_tab_progress_reporting on tab_progress_reporting;
 drop function notice_after_tab_progress_reporting();
@@ -311,7 +320,8 @@ CREATE TABLE parted_si_p_odd PARTITION OF parted_si FOR VALUES IN (1);
 -- https://postgr.es/m/18130-7a86a7356a75209d%40postgresql.org
 -- https://postgr.es/m/257696.1695670946%40sss.pgh.pa.us
 \set filename :abs_srcdir '/data/desc.data'
-COPY parted_si(id, data) FROM :'filename';
+\set command '\\COPY parted_si(id, data) FROM ' :'filename';
+:command
 
 -- An earlier bug (see commit b1ecb9b3fcf) could end up using a buffer from
 -- the wrong partition. This test is *not* guaranteed to trigger that bug, but
diff --git a/src/test/regress/sql/create_index.sql b/src/test/regress/sql/create_index.sql
index d49ce9f3007..1ac43c1301b 100644
--- a/src/test/regress/sql/create_index.sql
+++ b/src/test/regress/sql/create_index.sql
@@ -71,7 +71,8 @@ CREATE TABLE fast_emp4000 (
 );
 
 \set filename :abs_srcdir '/data/rect.data'
-COPY slow_emp4000 FROM :'filename';
+\set command '\\copy slow_emp4000 FROM ' :'filename';
+:command
 
 INSERT INTO fast_emp4000 SELECT * FROM slow_emp4000;
 
@@ -269,7 +270,8 @@ CREATE TABLE array_index_op_test (
 );
 
 \set filename :abs_srcdir '/data/array.data'
-COPY array_index_op_test FROM :'filename';
+\set command '\\copy array_index_op_test FROM ' :'filename';
+:command
 ANALYZE array_index_op_test;
 
 SELECT * FROM array_index_op_test WHERE i = '{NULL}' ORDER BY seqno;
diff --git a/src/test/regress/sql/create_view.sql b/src/test/regress/sql/create_view.sql
index 8838a40f7ab..4b09319ebda 100644
--- a/src/test/regress/sql/create_view.sql
+++ b/src/test/regress/sql/create_view.sql
@@ -23,7 +23,8 @@ CREATE TABLE real_city (
 );
 
 \set filename :abs_srcdir '/data/real_city.data'
-COPY real_city FROM :'filename';
+\set command '\\copy real_city FROM ' :'filename';
+:command
 ANALYZE real_city;
 
 SELECT *
diff --git a/src/test/regress/sql/hash_index.sql b/src/test/regress/sql/hash_index.sql
index 527024f7109..de49c0b85f9 100644
--- a/src/test/regress/sql/hash_index.sql
+++ b/src/test/regress/sql/hash_index.sql
@@ -26,10 +26,14 @@ CREATE TABLE hash_f8_heap (
 );
 
 \set filename :abs_srcdir '/data/hash.data'
-COPY hash_i4_heap FROM :'filename';
-COPY hash_name_heap FROM :'filename';
-COPY hash_txt_heap FROM :'filename';
-COPY hash_f8_heap FROM :'filename';
+\set command '\\copy hash_i4_heap FROM ' :'filename';
+:command
+\set command '\\copy hash_name_heap FROM ' :'filename';
+:command
+\set command '\\copy hash_txt_heap FROM ' :'filename';
+:command
+\set command '\\copy hash_f8_heap FROM ' :'filename';
+:command
 
 -- the data in this file has a lot of duplicates in the index key
 -- fields, leading to long bucket chains and lots of table expansion.
diff --git a/src/test/regress/sql/jsonb.sql b/src/test/regress/sql/jsonb.sql
index 6dae715afd8..aa320ba7be1 100644
--- a/src/test/regress/sql/jsonb.sql
+++ b/src/test/regress/sql/jsonb.sql
@@ -6,7 +6,8 @@ CREATE TABLE testjsonb (
 );
 
 \set filename :abs_srcdir '/data/jsonb.data'
-COPY testjsonb FROM :'filename';
+\set command '\\copy testjsonb FROM ' :'filename';
+:command
 
 -- Strings.
 SELECT '""'::jsonb;				-- OK.
diff --git a/src/test/regress/sql/largeobject.sql b/src/test/regress/sql/largeobject.sql
index a4aee02e3a4..721057c98c8 100644
--- a/src/test/regress/sql/largeobject.sql
+++ b/src/test/regress/sql/largeobject.sql
@@ -189,7 +189,8 @@ SELECT lo_unlink(loid) from lotest_stash_values;
 TRUNCATE lotest_stash_values;
 
 \set filename :abs_srcdir '/data/tenk.data'
-INSERT INTO lotest_stash_values (loid) SELECT lo_import(:'filename');
+\lo_import :filename
+INSERT INTO lotest_stash_values (loid) VALUES (:LASTOID);
 
 BEGIN;
 UPDATE lotest_stash_values SET fd=lo_open(loid, CAST(x'20000' | x'40000' AS integer));
@@ -219,8 +220,8 @@ SELECT lo_close(fd) FROM lotest_stash_values;
 END;
 
 \set filename :abs_builddir '/results/lotest.txt'
-SELECT lo_export(loid, :'filename') FROM lotest_stash_values;
-
+SELECT loid FROM lotest_stash_values \gset
+\lo_export :loid, :filename
 \lo_import :filename
 
 \set newloid :LASTOID
diff --git a/src/test/regress/sql/misc.sql b/src/test/regress/sql/misc.sql
index 165a2e175fb..08d7096e2cd 100644
--- a/src/test/regress/sql/misc.sql
+++ b/src/test/regress/sql/misc.sql
@@ -74,22 +74,26 @@ DROP TABLE tmp;
 -- copy
 --
 \set filename :abs_builddir '/results/onek.data'
-COPY onek TO :'filename';
+\set command '\\copy onek TO ' :'filename';
+:command
 
 CREATE TEMP TABLE onek_copy (LIKE onek);
 
-COPY onek_copy FROM :'filename';
+\set command '\\copy onek_copy FROM ' :'filename';
+:command
 
 SELECT * FROM onek EXCEPT ALL SELECT * FROM onek_copy;
 
 SELECT * FROM onek_copy EXCEPT ALL SELECT * FROM onek;
 
 \set filename :abs_builddir '/results/stud_emp.data'
-COPY BINARY stud_emp TO :'filename';
+\set command '\\COPY BINARY stud_emp TO ' :'filename';
+:command
 
 CREATE TEMP TABLE stud_emp_copy (LIKE stud_emp);
 
-COPY BINARY stud_emp_copy FROM :'filename';
+\set command '\\COPY BINARY stud_emp_copy FROM ' :'filename';
+:command
 
 SELECT * FROM stud_emp_copy;
 
diff --git a/src/test/regress/sql/password.sql b/src/test/regress/sql/password.sql
index 53e86b0b6ce..f8bf0d1323f 100644
--- a/src/test/regress/sql/password.sql
+++ b/src/test/regress/sql/password.sql
@@ -10,11 +10,11 @@ SET password_encryption = 'scram-sha-256'; -- ok
 
 -- consistency of password entries
 SET password_encryption = 'md5';
-CREATE ROLE regress_passwd1 PASSWORD 'role_pwd1';
-CREATE ROLE regress_passwd2 PASSWORD 'role_pwd2';
+CREATE ROLE regress_passwd1 PASSWORD 'role_pwd1Mahqu4eit2';
+CREATE ROLE regress_passwd2 PASSWORD 'role_pwd2Mahqu4eit2';
 SET password_encryption = 'scram-sha-256';
-CREATE ROLE regress_passwd3 PASSWORD 'role_pwd3';
-CREATE ROLE regress_passwd4 PASSWORD NULL;
+CREATE ROLE regress_passwd3 PASSWORD 'role_pwd3Mahqu4eit2';
+CREATE ROLE regress_passwd4 PASSWORD 'Mahqu4eit2FHS';
 
 -- check list of created entries
 --
@@ -42,14 +42,14 @@ ALTER ROLE regress_passwd2_new RENAME TO regress_passwd2;
 SET password_encryption = 'md5';
 
 -- encrypt with MD5
-ALTER ROLE regress_passwd2 PASSWORD 'foo';
+ALTER ROLE regress_passwd2 PASSWORD 'Mahqu4eit2FOO';
 -- already encrypted, use as they are
 ALTER ROLE regress_passwd1 PASSWORD 'md5cd3578025fe2c3d7ed1b9a9b26238b70';
 ALTER ROLE regress_passwd3 PASSWORD 'SCRAM-SHA-256$4096:VLK4RMaQLCvNtQ==$6YtlR4t69SguDiwFvbVgVZtuz6gpJQQqUMZ7IQJK5yI=:ps75jrHeYU4lXCcXI4O8oIdJ3eO8o2jirjruw9phBTo=';
 
 SET password_encryption = 'scram-sha-256';
 -- create SCRAM secret
-ALTER ROLE  regress_passwd4 PASSWORD 'foo';
+ALTER ROLE  regress_passwd4 PASSWORD 'Mahqu4eit2foo';
 -- already encrypted with MD5, use as it is
 CREATE ROLE regress_passwd5 PASSWORD 'md5e73a4b11df52a6068f8b39f90be36023';
 
diff --git a/src/test/regress/sql/psql.sql b/src/test/regress/sql/psql.sql
index f3bc6cd07e8..4e32f4bddba 100644
--- a/src/test/regress/sql/psql.sql
+++ b/src/test/regress/sql/psql.sql
@@ -1431,11 +1431,14 @@ CREATE TEMPORARY TABLE reload_output(
 );
 
 SELECT 1 AS a \g :g_out_file
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 SELECT 2 AS b\; SELECT 3 AS c\; SELECT 4 AS d \g :g_out_file
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 COPY (SELECT 'foo') TO STDOUT \; COPY (SELECT 'bar') TO STDOUT \g :g_out_file
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 
 SELECT line FROM reload_output ORDER BY lineno;
 TRUNCATE TABLE reload_output;
@@ -1452,17 +1455,20 @@ SELECT 1 AS a\; SELECT 2 AS b\; SELECT 3 AS c;
 -- COPY TO file
 -- The data goes to :g_out_file and the status to :o_out_file
 \set QUIET false
-COPY (SELECT unique1 FROM onek ORDER BY unique1 LIMIT 10) TO :'g_out_file';
+\set command '\\COPY (SELECT unique1 FROM onek ORDER BY unique1 LIMIT 10) TO ' :'g_out_file';
+:command
 -- DML command status
 UPDATE onek SET unique1 = unique1 WHERE false;
 \set QUIET true
 \o
 
 -- Check the contents of the files generated.
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
 TRUNCATE TABLE reload_output;
-COPY reload_output(line) FROM :'o_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'o_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
 TRUNCATE TABLE reload_output;
 
@@ -1475,10 +1481,12 @@ COPY (SELECT 'foo2') TO STDOUT \; COPY (SELECT 'bar2') TO STDOUT \g :g_out_file
 \o
 
 -- Check the contents of the files generated.
-COPY reload_output(line) FROM :'g_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'g_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
 TRUNCATE TABLE reload_output;
-COPY reload_output(line) FROM :'o_out_file';
+\set command '\\COPY reload_output(line) FROM ' :'o_out_file';
+:command
 SELECT line FROM reload_output ORDER BY lineno;
 
 DROP TABLE reload_output;
diff --git a/src/test/regress/sql/test_setup.sql b/src/test/regress/sql/test_setup.sql
index 1b2d434683b..b765c748b8c 100644
--- a/src/test/regress/sql/test_setup.sql
+++ b/src/test/regress/sql/test_setup.sql
@@ -135,7 +135,8 @@ CREATE TABLE onek (
 );
 
 \set filename :abs_srcdir '/data/onek.data'
-COPY onek FROM :'filename';
+\set command '\\copy onek FROM ' :'filename';
+:command
 VACUUM ANALYZE onek;
 
 CREATE TABLE onek2 AS SELECT * FROM onek;
@@ -161,7 +162,8 @@ CREATE TABLE tenk1 (
 );
 
 \set filename :abs_srcdir '/data/tenk.data'
-COPY tenk1 FROM :'filename';
+\set command '\\copy tenk1 FROM ' :'filename';
+:command
 VACUUM ANALYZE tenk1;
 
 CREATE TABLE tenk2 AS SELECT * FROM tenk1;
@@ -174,7 +176,8 @@ CREATE TABLE person (
 );
 
 \set filename :abs_srcdir '/data/person.data'
-COPY person FROM :'filename';
+\set command '\\copy person FROM ' :'filename';
+:command
 VACUUM ANALYZE person;
 
 CREATE TABLE emp (
@@ -183,7 +186,8 @@ CREATE TABLE emp (
 ) INHERITS (person);
 
 \set filename :abs_srcdir '/data/emp.data'
-COPY emp FROM :'filename';
+\set command '\\copy emp FROM ' :'filename';
+:command
 VACUUM ANALYZE emp;
 
 CREATE TABLE student (
@@ -191,7 +195,8 @@ CREATE TABLE student (
 ) INHERITS (person);
 
 \set filename :abs_srcdir '/data/student.data'
-COPY student FROM :'filename';
+\set command '\\copy student FROM ' :'filename';
+:command
 VACUUM ANALYZE student;
 
 CREATE TABLE stud_emp (
@@ -199,7 +204,8 @@ CREATE TABLE stud_emp (
 ) INHERITS (emp, student);
 
 \set filename :abs_srcdir '/data/stud_emp.data'
-COPY stud_emp FROM :'filename';
+\set command '\\copy stud_emp FROM ' :'filename';
+:command
 VACUUM ANALYZE stud_emp;
 
 CREATE TABLE road (
@@ -208,7 +214,8 @@ CREATE TABLE road (
 );
 
 \set filename :abs_srcdir '/data/streets.data'
-COPY road FROM :'filename';
+\set command '\\copy road FROM ' :'filename';
+:command
 VACUUM ANALYZE road;
 
 CREATE TABLE ihighway () INHERITS (road);
diff --git a/src/test/regress/sql/tsearch.sql b/src/test/regress/sql/tsearch.sql
index db2e09be94c..f6c6c3e1801 100644
--- a/src/test/regress/sql/tsearch.sql
+++ b/src/test/regress/sql/tsearch.sql
@@ -49,7 +49,8 @@ CREATE TABLE test_tsvector(
 );
 
 \set filename :abs_srcdir '/data/tsearch.data'
-COPY test_tsvector FROM :'filename';
+\set command '\\copy test_tsvector FROM ' :'filename';
+:command
 
 ANALYZE test_tsvector;
 
